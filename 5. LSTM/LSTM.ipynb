{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8CAWhRZk7Iz",
        "outputId": "3fb74369-9a24-4335-cd72-6ecc8529a2e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded all libraries\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import random\n",
        "from tensorflow.keras.models import load_model\n",
        "print('Loaded all libraries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "hU824DfylXV1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working for HDFCBANK.NS\n",
            "selected features ['COD', 'RSI_Close', 'DEMA_Close_F', 'OBV', 'Open', 'DEMA_Close_M', 'EMA_Close_F', 'lag1', 'SAR', 'VWAP']\n",
            "time steps:5,num_layers:1,units:96,activation:relu,dropout:0.1,rec.drop:0.3,lr:0.001\n",
            "Model loaded for HDFCBANK.NS\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "tickers = ['RELIANCE.NS','HINDUNILVR.NS','ITC.NS','IFBIND.NS','HDFCBANK.NS']\n",
        "ticker = tickers[4]\n",
        "\n",
        "data = pd.read_csv(f'StockData {ticker}.csv')\n",
        "\n",
        "# Select features and target\n",
        "if ticker == 'RELIANCE.NS':\n",
        "    features = [\"RSI_Close\", \"Open\", \"macd\", \"rpi\", \"lag1\", \"Nifty50\", \"AvgTrueRange\", \"DEMA_Close_F\", \"eps\", \"COD\"]\n",
        "    time_steps = 25\n",
        "    num_layers = 1\n",
        "    units = 128\n",
        "    activation = 'tanh'\n",
        "    dropout_rate = 0.3\n",
        "    recurrent_dropout_rate = 0.3\n",
        "    learning_rate = 0.01\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "elif ticker == 'HINDUNILVR.NS':\n",
        "    features = [\"COD\", \"RSI_Close\", \"DEMA_Close_F\", \"EMA_Close_F\", \"Open\", \"DEMA_Close_M\", \"SAR\", \"lag1\", \"OBV\", \"macd\"]\n",
        "    time_steps = 30\n",
        "    num_layers = 1\n",
        "    units = 128\n",
        "    activation = 'tanh'\n",
        "    dropout_rate = 0.1\n",
        "    recurrent_dropout_rate = 0.2\n",
        "    learning_rate = 0.01\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "elif ticker == 'ITC.NS':\n",
        "    features = [\"COD\", \"RSI_Close\", \"DEMA_Close_F\", \"Open\", \"EMA_Close_F\", \"Nifty50\", \"DEMA_Close_M\", \"macd\", \"up\", \"lag1\"]\n",
        "    time_steps = 15\n",
        "    num_layers = 1\n",
        "    units = 160\n",
        "    activation = 'tanh'\n",
        "    dropout_rate = 0.1\n",
        "    recurrent_dropout_rate = 0.2\n",
        "    learning_rate = 0.01\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "elif ticker == 'IFBIND.NS':\n",
        "    features = [\"COD\", \"RSI_Close\", \"DEMA_Close_F\", \"DEMA_Close_M\", \"EMA_Close_F\", \"Volume\", \"OBV\", \"Open\", \"Nifty50\", \"RVOL\"]\n",
        "    time_steps = 25\n",
        "    num_layers = 1\n",
        "    units = 128\n",
        "    activation = 'relu'\n",
        "    dropout_rate = 0.2\n",
        "    recurrent_dropout_rate = 0.4\n",
        "    learning_rate = 0.001\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "elif ticker == 'HDFCBANK.NS':\n",
        "    features = [\"COD\", \"RSI_Close\", \"DEMA_Close_F\", \"OBV\", \"Open\", \"DEMA_Close_M\", \"EMA_Close_F\", \"lag1\", \"SAR\", \"VWAP\"]\n",
        "    time_steps = 5\n",
        "    num_layers = 1\n",
        "    units = 96\n",
        "    activation = 'relu'\n",
        "    dropout_rate = 0.1\n",
        "    recurrent_dropout_rate = 0.3\n",
        "    learning_rate = 0.001\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "else:\n",
        "    raise ValueError(\"Ticker not recognized\")\n",
        "\n",
        "target = \"Close\"\n",
        "data = data[features + [target]]\n",
        "# Separate out the last entry for future use\n",
        "future = data.iloc[-1]\n",
        "data = data.iloc[:-1]\n",
        "# Set seeds for reproducibility\n",
        "seed = 2024\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Split into features and target\n",
        "X = scaled_data[:, :-1]\n",
        "y = scaled_data[:, -1]\n",
        "\n",
        "# Function to create sequences\n",
        "def create_sequences(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# Global test_days variable\n",
        "test_days = 60\n",
        "print(f'working for {ticker}')\n",
        "print(f'selected features {features}')\n",
        "print(f'time steps:{time_steps},num_layers:{num_layers},units:{units},activation:{activation},dropout:{dropout_rate},rec.drop:{recurrent_dropout_rate},lr:{learning_rate}')\n",
        "\n",
        "# Load model\n",
        "model = load_model(f'best_model_{ticker}.keras')\n",
        "print(f'Model loaded for {ticker}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "DNmUQvE8mnHT"
      },
      "outputs": [],
      "source": [
        "# # Hyper Parameter Tuning\n",
        "# print(f'working for {ticker}')\n",
        "# # Function to build and evaluate LSTM model\n",
        "# def build_model(hp):\n",
        "#     model = Sequential()\n",
        "    \n",
        "#     # Tune time steps\n",
        "#     time_steps = hp.Int('time_steps', min_value=5, max_value=30, step=5)\n",
        "#     X_seq, y_seq = create_sequences(X, y, time_steps)\n",
        "#     X_train, X_test = X_seq[:-test_days], X_seq[-test_days:]\n",
        "#     y_train, y_test = y_seq[:-test_days], y_seq[-test_days:]\n",
        "    \n",
        "#     num_layers = hp.Int('num_layers', min_value=1, max_value=1)\n",
        "    \n",
        "#     for i in range(num_layers):\n",
        "#         if i == 0:\n",
        "#             model.add(LSTM(\n",
        "#                 units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
        "#                 activation=hp.Choice('activation', values=['relu', 'tanh']),\n",
        "#                 return_sequences=True if i < num_layers - 1 else False,\n",
        "#                 input_shape=(time_steps, X_train.shape[2]),\n",
        "#                 recurrent_dropout=hp.Float(f'recurrent_dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)\n",
        "#             ))\n",
        "#         else:\n",
        "#             model.add(LSTM(\n",
        "#                 units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
        "#                 activation=hp.Choice('activation', values=['relu', 'tanh']),\n",
        "#                 return_sequences=True if i < num_layers - 1 else False,\n",
        "#                 recurrent_dropout=hp.Float(f'recurrent_dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)\n",
        "#             ))\n",
        "#         model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    \n",
        "#     model.add(Dense(1))\n",
        "    \n",
        "#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-2])\n",
        "#     hp_optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
        "    \n",
        "#     if hp_optimizer == 'adam':\n",
        "#         optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
        "#     elif hp_optimizer == 'rmsprop':\n",
        "#         optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp_learning_rate)\n",
        "#     else:\n",
        "#         optimizer = tf.keras.optimizers.SGD(learning_rate=hp_learning_rate)\n",
        "    \n",
        "#     model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    \n",
        "#     history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# # Initialize the tuner\n",
        "# tuner = kt.RandomSearch(build_model, objective='val_loss', max_trials=25, executions_per_trial=2, directory='my_dir', project_name=f'lstm_hyperparameter_tuning_{ticker}')\n",
        "\n",
        "# # Create sequences based on initial time steps\n",
        "# initial_time_steps = 10\n",
        "# X_seq, y_seq = create_sequences(X, y, initial_time_steps)\n",
        "\n",
        "# # Split the dataset\n",
        "# test_days = 60\n",
        "# X_train, X_test = X_seq[:-test_days], X_seq[-test_days:]\n",
        "# y_train, y_test = y_seq[:-test_days], y_seq[-test_days:]\n",
        "\n",
        "# # Search for best hyperparameters\n",
        "# tuner.search(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "# # Get the optimal hyperparameters\n",
        "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# print(f\"\"\"\n",
        "# The hyperparameter search is complete. The optimal configuration is:\n",
        "# - Time steps: {best_hps.get('time_steps')}\n",
        "# - Number of LSTM layers: {best_hps.get('num_layers')}\n",
        "# - Units per layer: {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}\n",
        "# - Dropout rate per layer: {[best_hps.get(f'dropout_{i}') for i in range(best_hps.get('num_layers'))]}\n",
        "# - Recurrent dropout per layer: {[best_hps.get(f'recurrent_dropout_{i}') for i in range(best_hps.get('num_layers'))]}\n",
        "# - Activation function: {best_hps.get('activation')}\n",
        "# - Learning rate: {best_hps.get('learning_rate')}\n",
        "# - Optimizer: {best_hps.get('optimizer')}\n",
        "# \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build LSTM with optimal Parameters manually\n",
        "# Create sequences\n",
        "X_seq, y_seq = create_sequences(X, y, time_steps)\n",
        "\n",
        "# Split the dataset\n",
        "test_days = 60\n",
        "X_train, X_test = X_seq[:-test_days], X_seq[-test_days:]\n",
        "y_train, y_test = y_seq[:-test_days], y_seq[-test_days:]\n",
        "\n",
        "# # Build the model\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(units=units, activation=activation, return_sequences=False, input_shape=(time_steps, X_train.shape[2]), recurrent_dropout=recurrent_dropout_rate))\n",
        "# model.add(Dropout(dropout_rate))\n",
        "# model.add(Dense(1))\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "# history = model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4AwuVT6pxXo",
        "outputId": "71882d1a-75e5-41b1-b2a3-0dad6006a544"
      },
      "outputs": [],
      "source": [
        "# # Build LSTM with Optimal Parameters (automatic)\n",
        "# # Optimal time steps\n",
        "# time_steps = best_hps.get('time_steps')\n",
        "# X_seq, y_seq = create_sequences(X, y, time_steps)\n",
        "\n",
        "# # Split the dataset\n",
        "# test_days = 60\n",
        "# X_train, X_test = X_seq[:-test_days], X_seq[-test_days:]\n",
        "# y_train, y_test = y_seq[:-test_days], y_seq[-test_days:]\n",
        "\n",
        "# # Build the final model with the optimal hyperparameters\n",
        "# model = Sequential()\n",
        "# for i in range(best_hps.get('num_layers')):\n",
        "#     if i == 0:\n",
        "#         model.add(LSTM(\n",
        "#             units=best_hps.get(f'units_{i}'),\n",
        "#             activation=best_hps.get('activation'),\n",
        "#             return_sequences=True if i < best_hps.get('num_layers') - 1 else False,\n",
        "#             input_shape=(time_steps, X_train.shape[2]),\n",
        "#             recurrent_dropout=best_hps.get(f'recurrent_dropout_{i}')\n",
        "#         ))\n",
        "#     else:\n",
        "#         model.add(LSTM(\n",
        "#             units=best_hps.get(f'units_{i}'),\n",
        "#             activation=best_hps.get('activation'),\n",
        "#             return_sequences=True if i < best_hps.get('num_layers') - 1 else False,\n",
        "#             recurrent_dropout=best_hps.get(f'recurrent_dropout_{i}')\n",
        "#         ))\n",
        "#     model.add(Dropout(best_hps.get(f'dropout_{i}')))\n",
        "\n",
        "# model.add(Dense(1))\n",
        "\n",
        "# # Compile the model\n",
        "# if best_hps.get('optimizer') == 'adam':\n",
        "#     optimizer = tf.keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate'))\n",
        "# elif best_hps.get('optimizer') == 'rmsprop':\n",
        "#     optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_hps.get('learning_rate'))\n",
        "# else:\n",
        "#     optimizer = tf.keras.optimizers.SGD(learning_rate=best_hps.get('learning_rate'))\n",
        "\n",
        "# model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Kz811bVz_X1S",
        "outputId": "230f98f1-eae0-4809-a78c-3a3b95ded8eb"
      },
      "outputs": [],
      "source": [
        "# plt.plot(history.history['loss'], label='Train Loss')\n",
        "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "ogRIfLSFqKes"
      },
      "outputs": [],
      "source": [
        "# model.save(f'best_model_{ticker}.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "I0ES0bmFqOPg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 96)                41088     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 96)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 97        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41185 (160.88 KB)\n",
            "Trainable params: 41185 (160.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "fWMzsV8h-LvM",
        "outputId": "fef0ae6d-76e1-4993-a347-b28f07120ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 0s 2ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Training RMSE: 22.959012556209892\n",
            "Training MAPE: 0.01258750517580725\n",
            "Testing RMSE: 25.360792714334845\n",
            "Testing MAPE: 0.013226295752486232\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# Inverse transform the target column separately\n",
        "y_train_inv = scaler.inverse_transform(np.hstack((np.zeros((y_train.shape[0], X_train.shape[2])), y_train.reshape(-1, 1))))[:, -1]\n",
        "y_test_inv = scaler.inverse_transform(np.hstack((np.zeros((y_test.shape[0], X_test.shape[2])), y_test.reshape(-1, 1))))[:, -1]\n",
        "\n",
        "# Make predictions on the training dataset\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_train_pred_inv = scaler.inverse_transform(np.hstack((np.zeros((y_train_pred.shape[0], X_train.shape[2])), y_train_pred)))[:, -1]\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_test_pred_inv = scaler.inverse_transform(np.hstack((np.zeros((y_test_pred.shape[0], X_test.shape[2])), y_test_pred)))[:, -1]\n",
        "\n",
        "# Calculate RMSE and MAPE for training data\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_inv, y_train_pred_inv))\n",
        "mape_train = mean_absolute_percentage_error(y_train_inv, y_train_pred_inv)\n",
        "\n",
        "# Calculate RMSE and MAPE for testing data\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv))\n",
        "mape_test = mean_absolute_percentage_error(y_test_inv, y_test_pred_inv)\n",
        "\n",
        "print(f\"Training RMSE: {rmse_train}\")\n",
        "print(f\"Training MAPE: {mape_train}\")\n",
        "print(f\"Testing RMSE: {rmse_test}\")\n",
        "print(f\"Testing MAPE: {mape_test}\")\n",
        "\n",
        "# Plot the actual vs. predicted values\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(y_test_inv, label='Actual')\n",
        "plt.plot(y_test_pred_inv, label='Predicted')\n",
        "plt.title('Actual vs Predicted Closing Prices')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.legend()\n",
        "\n",
        "# Save the plot before showing it\n",
        "plot_filename = f'{os.path.splitext(ticker)[0]}_TestPerformance_plot.png'\n",
        "plt.savefig(plot_filename, dpi=300, bbox_inches='tight')  # Save with high DPI and tight bounding box\n",
        "plt.close()\n",
        "# Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 96ms/step\n",
            "Future RMSE: 3.6875091708996024\n",
            "Future MAPE: 0.0021899923171338513\n",
            "Actual Value: 1683.800049\n",
            "Forecast : [1680.11253983]\n"
          ]
        }
      ],
      "source": [
        "# Set seeds for reproducibility\n",
        "seed = 2024\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Scale the future values\n",
        "future_scaled = scaler.transform([future])\n",
        "\n",
        "# Create sequence from the last `time_steps` entries of the training data\n",
        "last_sequence = scaled_data[-time_steps:, :-1]\n",
        "last_sequence = np.append(last_sequence, future_scaled[:, :-1], axis=0)[-time_steps:]\n",
        "\n",
        "# Reshape for prediction\n",
        "last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "\n",
        "# Predict the future value\n",
        "future_pred = model.predict(last_sequence)\n",
        "\n",
        "# Inverse transform the prediction\n",
        "future_pred_inv = scaler.inverse_transform(np.hstack((np.zeros((future_pred.shape[0], last_sequence.shape[2])), future_pred)))[:, -1]\n",
        "\n",
        "# Calculate RMSE and MAPE for the future value\n",
        "future_actual = future[target]\n",
        "rmse_future = np.sqrt(mean_squared_error([future_actual], future_pred_inv))\n",
        "mape_future = mean_absolute_percentage_error([future_actual], future_pred_inv)\n",
        "\n",
        "print(f\"Future RMSE: {rmse_future}\")\n",
        "print(f\"Future MAPE: {mape_future}\")\n",
        "print(f\"Actual Value: {future_actual}\")\n",
        "print(f\"Forecast : {future_pred_inv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Combine actual and predicted values into a DataFrame\n",
        "# results = pd.DataFrame({'Actual': y_test_inv,'Predicted': y_test_pred_inv})\n",
        "\n",
        "# # Save the results to a CSV file\n",
        "# results.to_csv(f'{ticker}_test_results.csv', index=False)\n",
        "# print(f\"Results saved to {ticker}_test_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "yc27iScMrcOX"
      },
      "outputs": [],
      "source": [
        "# import shap\n",
        "# # Create an explainer\n",
        "# explainer = shap.GradientExplainer(model, X_train)\n",
        "# # Compute SHAP values for the training and test sets\n",
        "# shap_values_train = explainer.shap_values(X_train)\n",
        "# shap_values_test = explainer.shap_values(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Set seeds for reproducibility\n",
        "# seed = 2024\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# tf.random.set_seed(seed)\n",
        "\n",
        "# # Ensure that the SHAP library is initialized\n",
        "# shap.initjs()\n",
        "\n",
        "# # Compute SHAP values for the future instance using the existing explainer\n",
        "# shap_values_future = explainer.shap_values(last_sequence)\n",
        "\n",
        "# # Aggregate SHAP values across all time steps for each feature\n",
        "# shap_values_aggregated = np.sum(shap_values_future[0], axis=0).flatten()\n",
        "\n",
        "# # Extract the feature values for the last time step (most recent values before prediction)\n",
        "# data_last_time_step = last_sequence[0, -1, :]  # Include all features\n",
        "\n",
        "# # Create a full feature row including the target to use scaler.inverse_transform\n",
        "# full_feature_row = np.zeros((1, len(features) + 1))\n",
        "# full_feature_row[0, :-1] = data_last_time_step\n",
        "\n",
        "# # Inverse transform the feature values to get them back to their original scale\n",
        "# full_feature_row_unscaled = scaler.inverse_transform(full_feature_row)\n",
        "# data_last_time_step_unscaled = full_feature_row_unscaled[0, :-1]  # Remove the appended zero for the target\n",
        "\n",
        "# # Calculate the expected value (base value) manually\n",
        "# expected_value_scaled = model.predict(X_train).mean()\n",
        "# expected_value_unscaled = scaler.inverse_transform([[0] * len(features) + [expected_value_scaled]])[0, -1]\n",
        "\n",
        "# # Use the forecasted value stored in future_pred_inv\n",
        "# future_prediction_unscaled = future_pred_inv\n",
        "\n",
        "# # Adjust SHAP values to match the forecast and expected value\n",
        "# scaling_factor = (future_prediction_unscaled - expected_value_unscaled) / shap_values_aggregated.sum()\n",
        "# shap_values_rescaled = shap_values_aggregated * scaling_factor\n",
        "\n",
        "# # Create the waterfall plot with unscaled data\n",
        "# plt.title('SHAP Waterfall Breakdown')\n",
        "# shap.waterfall_plot(shap.Explanation(values=shap_values_rescaled, base_values=expected_value_unscaled, data=data_last_time_step_unscaled, \n",
        "#                                      feature_names=features),max_display=len(features),\n",
        "#                     show=False  # Disable automatic display\n",
        "#                     )\n",
        "\n",
        "# # Save the plot with a proper resolution\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_SHAPWaterfall_performance_plot.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "# plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kW9e4zEHrk8y",
        "outputId": "8d9c5471-a1bf-445e-fea1-1f089b056c29"
      },
      "outputs": [],
      "source": [
        "# # Aggregate SHAP values over time steps by summing absolute values\n",
        "# shap_values_train_aggregated = np.sum(np.abs(shap_values_train), axis=1).reshape(X_train.shape[0], -1)\n",
        "# shap_values_test_aggregated = np.sum(np.abs(shap_values_test), axis=1).reshape(X_test.shape[0], -1)\n",
        "\n",
        "# # Aggregate the corresponding features by taking the mean across time steps\n",
        "# X_train_aggregated = np.mean(X_train, axis=1)\n",
        "# X_test_aggregated = np.mean(X_test, axis=1)\n",
        "\n",
        "# # Plot the summary plot for feature importance (Train Data)\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# shap.summary_plot(shap_values_train_aggregated, features=X_train_aggregated, feature_names=features, show=False)\n",
        "# plt.title('SHAP Summary Plot for Training Data')\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_SHAPSummaryTrain_performance_plot.png'\n",
        "# plt.savefig(plot_filename)\n",
        "# # plt.show()\n",
        "# plt.close()\n",
        "\n",
        "\n",
        "# # Plot the summary plot for feature importance (Test Data)\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# shap.summary_plot(shap_values_test_aggregated, features=X_test_aggregated, feature_names=features, show=False)\n",
        "# plt.title('SHAP Summary Plot for Test Data')\n",
        "# plt.title('SHAP Summary Plot for Test Data')\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_SHAPSummaryTest_performance_plot.png'\n",
        "# plt.savefig(plot_filename)\n",
        "# # plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G1gVNB-zGqo-",
        "outputId": "19b9f81d-1547-421a-979f-53b51a7a1aae"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# sns.set(style=\"whitegrid\")\n",
        "\n",
        "# # Extract and reshape SHAP values for each feature and time step\n",
        "# time_steps = X_train.shape[1]\n",
        "\n",
        "# def plot_shap_values_over_time(shap_values, X, feature_names, title_prefix, save_path=None):\n",
        "#     num_features = len(feature_names)\n",
        "#     time_steps = X.shape[1]\n",
        "    \n",
        "#     fig, axes = plt.subplots(nrows=num_features // 2 + num_features % 2, ncols=5, figsize=(30, num_features * 2))\n",
        "#     axes = axes.flatten()\n",
        "    \n",
        "#     for i, feature in enumerate(feature_names):\n",
        "#         shap_values_feature = np.array([shap[:, i] for shap in shap_values]).reshape(-1, time_steps)\n",
        "#         mean_shap_values = np.mean(np.abs(shap_values_feature), axis=0)\n",
        "        \n",
        "#         # Normalize the mean SHAP values to the range [0, 1]\n",
        "#         mean_shap_values = (mean_shap_values - mean_shap_values.min()) / (mean_shap_values.max() - mean_shap_values.min())\n",
        "        \n",
        "#         time_steps_labels = [f't-{j}' for j in range(time_steps-1, -1, -1)]\n",
        "        \n",
        "#         sns.barplot(ax=axes[i], x=time_steps_labels, y=mean_shap_values, dodge=False, palette='Blues_d', legend=False)\n",
        "#         axes[i].set_title(f'{title_prefix} SHAP Values for {feature} Over Different Time Steps')\n",
        "#         axes[i].set_xlabel('Time Steps')\n",
        "#         axes[i].set_ylabel('Normalized Mean |SHAP Value|')\n",
        "\n",
        "#     # Hide any empty subplots\n",
        "#     for j in range(i + 1, len(axes)):\n",
        "#         fig.delaxes(axes[j])\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     # Save the plot with the specified file name\n",
        "#     if save_path:\n",
        "#         plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    \n",
        "#     # Display the plot\n",
        "#     # plt.show()\n",
        "\n",
        "# # Plot SHAP values for each feature in the training data\n",
        "# plot_shap_values_over_time(shap_values_test, X_test, features, 'Test', save_path=f'{os.path.splitext(ticker)[0]}_SHAPSequence_plot.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Convert numpy arrays to tensors\n",
        "# X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "# X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "\n",
        "# # Function to compute saliency map\n",
        "# def compute_saliency_map(model, X):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         tape.watch(X)\n",
        "#         predictions = model(X)\n",
        "#     gradients = tape.gradient(predictions, X)\n",
        "#     return tf.abs(gradients).numpy()\n",
        "\n",
        "# # Compute saliency maps for test data\n",
        "# saliency_maps_test = compute_saliency_map(model, X_test_tensor)\n",
        "# saliency_maps_train = compute_saliency_map(model, X_train_tensor)\n",
        "\n",
        "# def plot_saliency_map(saliency_map, feature_names, title=\"Saliency Map\"):\n",
        "#     mean_saliency_map = np.mean(saliency_map, axis=0)  # Average over samples\n",
        "#     plt.figure(figsize=(12, 12))\n",
        "#     plt.imshow(mean_saliency_map.T, cmap='hot', interpolation='nearest', aspect='auto')\n",
        "#     plt.colorbar(label='Saliency Value')\n",
        "#     plt.title(title)\n",
        "#     plt.xlabel('Time Steps')\n",
        "#     plt.ylabel('Features')\n",
        "#     plt.xticks(ticks=np.arange(saliency_map.shape[1]), labels=[f't-{i}' for i in range(saliency_map.shape[1]-1, -1, -1)])\n",
        "#     plt.yticks(ticks=np.arange(len(feature_names)), labels=feature_names)\n",
        "\n",
        "# # Plot saliency map for the test data\n",
        "# plot_saliency_map(saliency_maps_test, features, title=\"Saliency Map for Test Data\")\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_SaliencyTest_plot.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "# # plt.show()\n",
        "# plt.close()\n",
        "\n",
        "# # Plot saliency map for the train data\n",
        "# plot_saliency_map(saliency_maps_train, features, title=\"Saliency Map for Train Data\")\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_SaliencyTrain_plot.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "# # plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import lime\n",
        "# import lime.lime_tabular\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.metrics import r2_score\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Aggregate the training data over time steps by averaging\n",
        "# X_train_aggregated = np.mean(X_train, axis=1)\n",
        "\n",
        "# # Define the LIME explainer\n",
        "# explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
        "#     training_data=X_train_aggregated,\n",
        "#     feature_names=features,\n",
        "#     class_names=['Price'],\n",
        "#     mode='regression'\n",
        "# )\n",
        "\n",
        "# # Scale the future values\n",
        "# future_scaled = scaler.transform([future])\n",
        "\n",
        "# # Create sequence from the last `time_steps` entries of the training data\n",
        "# last_sequence = scaled_data[-time_steps:, :-1]\n",
        "# last_sequence = np.append(last_sequence, future_scaled[:, :-1], axis=0)[-time_steps:]\n",
        "\n",
        "# # Aggregate the last sequence over time steps by averaging\n",
        "# last_sequence_aggregated = np.mean(last_sequence, axis=0).reshape(1, -1)\n",
        "\n",
        "# # Define a function for LIME to use linear regression as a surrogate model\n",
        "# def predict_with_surrogate(data):\n",
        "#     surrogate_model = LinearRegression()\n",
        "#     surrogate_model.fit(X_train_aggregated, y_train)\n",
        "#     return surrogate_model.predict(data)\n",
        "\n",
        "# # Compute LIME explanations for the \"future\" instance\n",
        "# exp = explainer_lime.explain_instance(last_sequence_aggregated[0], predict_with_surrogate, num_features=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from scipy.stats import pearsonr\n",
        "# # Set seeds for reproducibility\n",
        "# seed = 2024\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# tf.random.set_seed(seed)\n",
        "\n",
        "# # Generate perturbations\n",
        "# num_samples = 5000\n",
        "# perturbations = explainer_lime.random_state.normal(\n",
        "#     loc=last_sequence_aggregated[0], scale=0.1, size=(num_samples, last_sequence_aggregated.shape[1])\n",
        "# )\n",
        "\n",
        "# # Reshape perturbations to the shape expected by the LSTM model (num_samples, time_steps, num_features)\n",
        "# perturbations_reshaped = np.repeat(perturbations[:, np.newaxis, :], time_steps, axis=1)\n",
        "\n",
        "# # Get predictions from the original model (LSTM model)\n",
        "# y_original = model.predict(perturbations_reshaped)\n",
        "\n",
        "# # Fit the surrogate model on the perturbed data and predict\n",
        "# y_surrogate = predict_with_surrogate(perturbations)\n",
        "\n",
        "# # Calculate correlation and square it to approximate R-squared\n",
        "# correlation, _ = pearsonr(y_original.flatten(), y_surrogate)\n",
        "# r_squared_approx = correlation**2\n",
        "# print(f\"R-squared Approximation (Correlation Squared): {r_squared_approx}\")\n",
        "\n",
        "# # Plot the explanation\n",
        "# fig = exp.as_pyplot_figure()\n",
        "# plt.title(f'Prediction: 2935.20 \\n Explanation fit: {r_squared_approx:.2f}')\n",
        "# plt.xlabel('Contribution to Prediction', fontsize=8)\n",
        "# plt.ylabel('Feature', fontsize=8)\n",
        "# plt.gcf().set_size_inches(7, 5.8)\n",
        "# plt.tight_layout()\n",
        "# # Save the plot before showing it\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_LIME.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "# # plt.show()\n",
        "# plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LinearRegression\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Plotting the R-squared values against each input feature\n",
        "# def plot_r_squared_vs_feature(X_data, r_squared_values, dataset_type):\n",
        "#     n_features = X_data.shape[1]\n",
        "#     fig, axs = plt.subplots(2, 5, figsize=(20, 10))  # 2x5 arrangement\n",
        "\n",
        "#     for i in range(n_features):\n",
        "#         row, col = divmod(i, 5)\n",
        "        \n",
        "#         # Scatter plot for R-squared values vs feature values\n",
        "#         sns.scatterplot(x=X_data[:, i], y=r_squared_values, ax=axs[row, col])\n",
        "        \n",
        "#         # Fit a regression line\n",
        "#         lin_reg = LinearRegression()\n",
        "#         feature_values = X_data[:, i].reshape(-1, 1)\n",
        "#         lin_reg.fit(feature_values, r_squared_values)\n",
        "#         reg_line = lin_reg.predict(feature_values)\n",
        "        \n",
        "#         # Plot the regression line\n",
        "#         sns.lineplot(x=X_data[:, i], y=reg_line, color='red', ax=axs[row, col])\n",
        "        \n",
        "#         axs[row, col].set_title(f'{features[i]} vs R-squared ({dataset_type})')\n",
        "#         axs[row, col].set_xlabel(f'{features[i]}')\n",
        "#         axs[row, col].set_ylabel('R-squared')\n",
        "    \n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Plot for train data\n",
        "# plot_r_squared_vs_feature(X_train_aggregated, train_r_squared, 'Train')\n",
        "\n",
        "# # Plot for test data\n",
        "# plot_r_squared_vs_feature(X_test_aggregated, test_r_squared, 'Test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 0s/step\n",
            "2/2 [==============================] - 0s 0s/step\n",
            "2/2 [==============================] - 0s 0s/step\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "2/2 [==============================] - 0s 0s/step\n",
            "2/2 [==============================] - 0s 0s/step\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "2/2 [==============================] - 0s 0s/step\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "2/2 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "def feature_ablation(model, X, y, feature_names):\n",
        "    base_score = mean_squared_error(y, model.predict(X))\n",
        "    scores = []\n",
        "\n",
        "    for i in range(X.shape[-1]):\n",
        "        X_ablated = X.copy()\n",
        "        X_ablated[:, :, i] = 0  # Ablate the feature\n",
        "        score = mean_squared_error(y, model.predict(X_ablated))\n",
        "        scores.append(base_score - score)\n",
        "\n",
        "    plt.barh(feature_names, scores)\n",
        "    plt.xlabel('Ablation Importance')\n",
        "    plt.ylabel('Features')\n",
        "    plt.title('Feature Ablation Importance')\n",
        "    # Save the plot before showing it\n",
        "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "    # plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Compute feature ablation scores with actual feature names\n",
        "plot_filename = f'{os.path.splitext(ticker)[0]}_FeatureAblation_plot.png'\n",
        "feature_ablation(model, X_test, y_test, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Feature level Attention\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# input_features = len(features)\n",
        "\n",
        "# # Define the LSTM model with attention over features\n",
        "# input_layer = tf.keras.Input(shape=(time_steps, input_features))\n",
        "# lstm_layer = tf.keras.layers.LSTM(units = units,activation = activation, recurrent_dropout=recurrent_dropout_rate, return_sequences=True)(input_layer)\n",
        "\n",
        "# # Reshape LSTM output to (batch_size, time_steps, features)\n",
        "# # Then apply attention over the features\n",
        "# attention_scores = tf.keras.layers.Dense(input_features, activation='softmax')(lstm_layer)\n",
        "\n",
        "# # Apply attention scores to the input features\n",
        "# context_vector = tf.keras.layers.Multiply()([attention_scores, input_layer])\n",
        "\n",
        "# # Summarize the context vector to get a single vector\n",
        "# context_vector = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(context_vector)\n",
        "\n",
        "# # Add a dense layer for output\n",
        "# output_layer = tf.keras.layers.Dense(1)(context_vector)\n",
        "\n",
        "# model_with_attention = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "# model_with_attention.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# # Train the model\n",
        "# model_with_attention.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "# # Extract attention weights\n",
        "# attention_model = tf.keras.Model(inputs=model_with_attention.input, outputs=attention_scores)\n",
        "# attention_weights = attention_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Average attention across all test samples to get a general view\n",
        "# average_attention_weights = attention_weights.mean(axis=0)  # Shape will be (time_steps, features)\n",
        "\n",
        "# # Plot average attention weights\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(average_attention_weights.T, cmap='hot', annot=False)\n",
        "# plt.title('Attention-based Feature Importance Over Time')\n",
        "# plt.xlabel('Time Steps')\n",
        "# plt.ylabel('Features')\n",
        "# # Save the plot before showing it\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_Global Attention.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "# # plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from tensorflow.keras.layers import Input, Dense, LSTM, Permute, Reshape, multiply\n",
        "# from tensorflow.keras.models import Model\n",
        "\n",
        "# # Adjusted attention function\n",
        "# def attention(inputs, SHAPE):\n",
        "#     n_steps = int(inputs.shape[1]) \n",
        "#     a = Permute((2, 1))(inputs) \n",
        "#     a = Dense(n_steps, activation='softmax', name='attention_vec')(a)\n",
        "#     a = Permute((2, 1))(a)\n",
        "#     output_attention_mul = multiply([inputs, a])\n",
        "#     return output_attention_mul\n",
        "\n",
        "# # Function to create the model\n",
        "# def create_model(optimizer=\"adam\", dropout=0.3, dense_nparams1=128, lr=0.01, n_wind=25):\n",
        "#     input_layer = Input(shape=(n_wind, n_features)) \n",
        "#     x = attention(input_layer, n_features)\n",
        "#     x = LSTM(dense_nparams1, activation='tanh', return_sequences=False, recurrent_dropout=dropout)(x)\n",
        "#     preds = Dense(1)(x)\n",
        "#     model = Model(inputs=input_layer, outputs=preds)\n",
        "#     model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "#     return model\n",
        "\n",
        "# n_features = len(features)  # Number of features\n",
        "# n_wind = time_steps  # Number of time steps\n",
        "# model = create_model(optimizer=\"adam\", dropout=dropout_rate, dense_nparams1=units, lr=learning_rate, n_wind=n_wind)\n",
        "# model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test)) \n",
        "\n",
        "# # Predict the future value using the trained model\n",
        "# last_sequence = scaled_data[-n_wind:, :-1]  # Last sequence from training data\n",
        "# last_sequence = np.append(last_sequence, future_scaled[:, :-1], axis=0)[-n_wind:]  # Include the future data\n",
        "# last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "\n",
        "# # Predict the future value\n",
        "# future_pred = model.predict(last_sequence)\n",
        "# future_pred_inv = scaler.inverse_transform(np.hstack((np.zeros((future_pred.shape[0], last_sequence.shape[2])), future_pred)))[:, -1]\n",
        "\n",
        "# # Extract the attention activations for the future forecast\n",
        "# extract_attention_model = Model(inputs=model.input, outputs=model.get_layer('attention_vec').output)\n",
        "# future_attention = extract_attention_model.predict(last_sequence)\n",
        "\n",
        "# # Reshape the activations for plotting\n",
        "# future_attention_reshaped = future_attention.reshape(last_sequence.shape[0], last_sequence.shape[2], last_sequence.shape[1])\n",
        "\n",
        "# # Extract the attention activations for the future forecast\n",
        "# extract_attention_model = Model(inputs=model.input, outputs=model.get_layer('attention_vec').output)\n",
        "# future_attention = extract_attention_model.predict(last_sequence)\n",
        "\n",
        "# # Reshape the activations for plotting\n",
        "# future_attention_reshaped = future_attention.reshape(last_sequence.shape[0], last_sequence.shape[2], last_sequence.shape[1])\n",
        "\n",
        "# import matplotlib.colors as mcolors\n",
        "\n",
        "# # Custom normalization that amplifies small values\n",
        "# class CustomNormalize(mcolors.Normalize):\n",
        "#     def __init__(self, vmin=None, vmax=None, clip=False):\n",
        "#         super().__init__(vmin, vmax, clip)\n",
        "#         self.scaling_factor = 10  # Amplification factor for small values\n",
        "\n",
        "#     def __call__(self, value, clip=None):\n",
        "#         return np.ma.masked_array(value * self.scaling_factor, mask=np.isnan(value))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Calculate RMSE and MAPE for the future value\n",
        "# future_actual = future[target]\n",
        "# rmse_future = np.sqrt(mean_squared_error([future_actual], future_pred_inv))\n",
        "# mape_future = mean_absolute_percentage_error([future_actual], future_pred_inv)\n",
        "\n",
        "# print(f\"Future RMSE: {rmse_future}\")\n",
        "# print(f\"Future MAPE: {mape_future}\")\n",
        "# print(f\"Actual Value: {future_actual}\")\n",
        "# print(f\"Forecast : {future_pred_inv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# # Inverse transform the target column separately\n",
        "# y_train_inv = scaler.inverse_transform(np.hstack((np.zeros((y_train.shape[0], X_train.shape[2])), y_train.reshape(-1, 1))))[:, -1]\n",
        "# y_test_inv = scaler.inverse_transform(np.hstack((np.zeros((y_test.shape[0], X_test.shape[2])), y_test.reshape(-1, 1))))[:, -1]\n",
        "\n",
        "# # Make predictions on the training dataset\n",
        "# y_train_pred = model.predict(X_train)\n",
        "# y_train_pred_inv = scaler.inverse_transform(np.hstack((np.zeros((y_train_pred.shape[0], X_train.shape[2])), y_train_pred)))[:, -1]\n",
        "\n",
        "# # Make predictions on the test dataset\n",
        "# y_test_pred = model.predict(X_test)\n",
        "# y_test_pred_inv = scaler.inverse_transform(np.hstack((np.zeros((y_test_pred.shape[0], X_test.shape[2])), y_test_pred)))[:, -1]\n",
        "\n",
        "# # Calculate RMSE and MAPE for training data\n",
        "# rmse_train = np.sqrt(mean_squared_error(y_train_inv, y_train_pred_inv))\n",
        "# mape_train = mean_absolute_percentage_error(y_train_inv, y_train_pred_inv)\n",
        "\n",
        "# # Calculate RMSE and MAPE for testing data\n",
        "# rmse_test = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv))\n",
        "# mape_test = mean_absolute_percentage_error(y_test_inv, y_test_pred_inv)\n",
        "\n",
        "# print(f\"Training RMSE: {rmse_train}\")\n",
        "# print(f\"Training MAPE: {mape_train}\")\n",
        "# print(f\"Testing RMSE: {rmse_test}\")\n",
        "# print(f\"Testing MAPE: {mape_test}\")\n",
        "\n",
        "# # Plot the actual vs. predicted values\n",
        "# plt.figure(figsize=(14, 7))\n",
        "# plt.plot(y_test_inv, label='Actual')\n",
        "# plt.plot(y_test_pred_inv, label='Predicted')\n",
        "# plt.title('Actual vs Predicted Closing Prices')\n",
        "# plt.xlabel('Time')\n",
        "# plt.ylabel('Closing Price')\n",
        "# plt.legend()\n",
        "\n",
        "# # Save the plot before showing it\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_TestPerformanceWithAttention_plot.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')  # Save with high DPI and tight bounding box\n",
        "# plt.close()\n",
        "# # Show the plot\n",
        "# # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Plot the attention map for the chosen future forecast\n",
        "# fig, axs = plt.subplots(3, 1, figsize=(10, 10), gridspec_kw={'height_ratios': [2, 3, 3]})\n",
        "\n",
        "# # 1. Time-Averaged Contribution Barplot\n",
        "# average_contribution = np.mean(future_attention_reshaped[0], axis=1)\n",
        "# axs[0].bar(range(len(average_contribution)), average_contribution, color='black')\n",
        "# axs[0].set_xticks(range(n_features))\n",
        "# axs[0].set_xticklabels(features, rotation=90)\n",
        "# axs[0].set_ylabel('Average Contribution')\n",
        "# axs[0].set_title('Time-Averaged Contribution of Each Feature')\n",
        "\n",
        "# # Reverse the time steps for better visualization\n",
        "# time_steps_reversed = list(range(n_wind))[::-1]\n",
        "\n",
        "# # 2. Attention Activations Heatmap\n",
        "# sns.heatmap(future_attention_reshaped[0], cmap='magma', ax=axs[1], norm=CustomNormalize(), cbar_kws={'label': 'Activation Level'})\n",
        "# axs[1].set_yticks(range(n_features))\n",
        "# axs[1].set_yticklabels(features, rotation=0)\n",
        "# axs[1].set_xticks(range(n_wind))\n",
        "# axs[1].set_xticklabels(time_steps_reversed)\n",
        "# axs[1].set_ylabel('Features')\n",
        "# axs[1].set_title('Attention Activations Heatmap')\n",
        "\n",
        "# # 3. Input Features Heatmap\n",
        "# sns.heatmap(last_sequence[0].T, cmap='coolwarm', ax=axs[2])\n",
        "# axs[2].set_yticks(range(n_features))\n",
        "# axs[2].set_yticklabels(features, rotation=0)\n",
        "# axs[2].set_xticks(range(n_wind))\n",
        "# axs[2].set_xticklabels(time_steps_reversed)\n",
        "# axs[2].set_ylabel('Features')\n",
        "# axs[2].set_title('Input Features Heatmap')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# # Save the plot before showing it\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_Local Attention.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "# # plt.show()\n",
        "# plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "# from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# # Extract the unscaled test data from the original data\n",
        "# original_features_test = data.iloc[-len(y_test):][features].values\n",
        "# original_target_test = data.iloc[-len(y_test):][target].values\n",
        "\n",
        "# # Define custom colormap: Red -> Orange -> Blue\n",
        "# colors = [(0, 0, 1), (1, 0.65, 0), (1, 0, 0)]  # Blue -> Orange -> Red\n",
        "# n_bins = 100  # Number of color levels\n",
        "# cmap_name = 'custom_cmap'\n",
        "# cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "\n",
        "# # Set the seaborn style\n",
        "# sns.set(style=\"whitegrid\")\n",
        "\n",
        "# # Create a figure with subplots\n",
        "# fig, axes = plt.subplots(nrows=len(features) + 1, ncols=1, figsize=(8, 18), sharex=True)\n",
        "\n",
        "# # Plot actual closing prices\n",
        "# axes[0].plot(original_target_test, label='Actual Price', color='black', linewidth=2)\n",
        "# axes[0].plot(y_test_pred_inv, label='Predicted Price', color='red',linewidth=2)\n",
        "# axes[0].set_title('Actual Closing Price Over Time', fontsize=14)\n",
        "# axes[0].set_ylabel('Price', fontsize=12)\n",
        "# axes[0].legend(loc='lower right',fontsize=9,frameon=False)\n",
        "\n",
        "# # Plot each feature with SHAP values\n",
        "# for i, feature in enumerate(features):\n",
        "#     axes[i + 1].plot(original_features_test[:, i], color='black', marker='o', markersize=4, label=feature, linestyle='-', linewidth=1.5)\n",
        "\n",
        "#     # Normalize SHAP values for color mapping\n",
        "#     shap_values = shap_values_test_aggregated[:, i]\n",
        "#     norm_shap_values = (shap_values - np.min(shap_values)) / (np.max(shap_values) - np.min(shap_values))\n",
        "\n",
        "#     # Apply custom colormap to the background\n",
        "#     for j in range(len(shap_values)):\n",
        "#         color = cmap(norm_shap_values[j])\n",
        "#         axes[i + 1].axvspan(j - 0.5, j + 0.5, color=color, alpha=0.6)\n",
        "        \n",
        "#     axes[i + 1].set_title(f'{feature} (SHAP Impact)', fontsize=14)\n",
        "#     axes[i + 1].set_ylabel(feature, fontsize=12)\n",
        "#     axes[i + 1].legend().set_visible(False)\n",
        "\n",
        "# # Adjust the overall layout\n",
        "# fig.tight_layout(rect=[0, 0, 0.95, 1])\n",
        "# fig.subplots_adjust(hspace=0.3)\n",
        "\n",
        "# # Add color bar for SHAP values\n",
        "# cbar_ax = fig.add_axes([0.96, 0.15, 0.02, 0.7])\n",
        "# norm = plt.Normalize(0, 1)\n",
        "# sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "# sm.set_array([])\n",
        "# cbar = fig.colorbar(sm, cax=cbar_ax)\n",
        "# cbar.set_label('Normalized SHAP Value', fontsize=10)\n",
        "# cbar.set_ticks([0, 1])\n",
        "# # cbar.set_ticklabels(['Low (Blue)', 'Medium (Orange)', 'High (Red)'])\n",
        "# # Save the plot before showing it\n",
        "# plot_filename = f'{os.path.splitext(ticker)[0]}_SHAP over time.png'\n",
        "# plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "# # plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# # Function to apply perturbation based on SHAP or Saliency Maps\n",
        "# def apply_perturbation(X, map_values, threshold=0.7, method='swap'):\n",
        "#     X_perturbed = np.copy(X)\n",
        "#     for i in range(X.shape[0]):\n",
        "#         # Identify important time steps based on SHAP/Saliency map\n",
        "#         importance = np.abs(map_values[i])\n",
        "#         important_indices = np.where(importance > np.percentile(importance, 100 * threshold))[0]\n",
        "        \n",
        "#         for idx in important_indices:\n",
        "#             if method == 'swap':\n",
        "#                 # Swap with neighboring time steps\n",
        "#                 start = max(0, idx - 1)\n",
        "#                 end = min(X.shape[1], idx + 2)\n",
        "#                 X_perturbed[i, start:end] = X_perturbed[i, start:end][::-1]\n",
        "#             elif method == 'mean':\n",
        "#                 # Replace with mean value\n",
        "#                 start = max(0, idx - 1)\n",
        "#                 end = min(X.shape[1], idx + 2)\n",
        "#                 mean_value = np.mean(X_perturbed[i, start:end], axis=0)\n",
        "#                 X_perturbed[i, start:end] = mean_value\n",
        "#             elif method == 'zero':\n",
        "#                 # Set to zero\n",
        "#                 X_perturbed[i, idx] = 0\n",
        "#             elif method == 'permute':\n",
        "#                 # Permute (randomize) the important indices\n",
        "#                 np.random.shuffle(X_perturbed[i, idx])\n",
        "#     return X_perturbed\n",
        "\n",
        "# # Compute SHAP values (already done earlier)\n",
        "# # shap_values_train = ...\n",
        "# # shap_values_test = ...\n",
        "\n",
        "# # Compute Saliency Maps (already done earlier)\n",
        "# # saliency_maps_train = ...\n",
        "# # saliency_maps_test = ...\n",
        "\n",
        "# # Evaluate SHAP method\n",
        "# X_test_shap_swap = apply_perturbation(X_test, shap_values_test, threshold=0.7, method='swap')\n",
        "# X_test_shap_mean = apply_perturbation(X_test, shap_values_test, threshold=0.7, method='mean')\n",
        "# X_test_shap_zero = apply_perturbation(X_test, shap_values_test, threshold=0.7, method='zero')\n",
        "# X_test_shap_permute = apply_perturbation(X_test, shap_values_test, threshold=0.7, method='permute')\n",
        "\n",
        "# # Evaluate Saliency method\n",
        "# X_test_saliency_swap = apply_perturbation(X_test, saliency_maps_test, threshold=0.7, method='swap')\n",
        "# X_test_saliency_mean = apply_perturbation(X_test, saliency_maps_test, threshold=0.7, method='mean')\n",
        "# X_test_saliency_zero = apply_perturbation(X_test, saliency_maps_test, threshold=0.7, method='zero')\n",
        "# X_test_saliency_permute = apply_perturbation(X_test, saliency_maps_test, threshold=0.7, method='permute')\n",
        "\n",
        "# # Predict on original data\n",
        "# y_test_pred = model.predict(X_test)\n",
        "\n",
        "# # Predict on SHAP-perturbed datasets\n",
        "# y_test_pred_shap_swap = model.predict(X_test_shap_swap)\n",
        "# y_test_pred_shap_mean = model.predict(X_test_shap_mean)\n",
        "# y_test_pred_shap_zero = model.predict(X_test_shap_zero)\n",
        "# y_test_pred_shap_permute = model.predict(X_test_shap_permute)\n",
        "\n",
        "# # Predict on Saliency-perturbed datasets\n",
        "# y_test_pred_saliency_swap = model.predict(X_test_saliency_swap)\n",
        "# y_test_pred_saliency_mean = model.predict(X_test_saliency_mean)\n",
        "# y_test_pred_saliency_zero = model.predict(X_test_saliency_zero)\n",
        "# y_test_pred_saliency_permute = model.predict(X_test_saliency_permute)\n",
        "\n",
        "# # Calculate RMSE and MAPE for SHAP-perturbed datasets\n",
        "# rmse_shap_swap = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_shap_swap))\n",
        "# mape_shap_swap = mean_absolute_percentage_error(y_test_inv, y_test_pred_shap_swap)\n",
        "\n",
        "# rmse_shap_mean = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_shap_mean))\n",
        "# mape_shap_mean = mean_absolute_percentage_error(y_test_inv, y_test_pred_shap_mean)\n",
        "\n",
        "# rmse_shap_zero = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_shap_zero))\n",
        "# mape_shap_zero = mean_absolute_percentage_error(y_test_inv, y_test_pred_shap_zero)\n",
        "\n",
        "# rmse_shap_permute = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_shap_permute))\n",
        "# mape_shap_permute = mean_absolute_percentage_error(y_test_inv, y_test_pred_shap_permute)\n",
        "\n",
        "# # Calculate RMSE and MAPE for Saliency-perturbed datasets\n",
        "# rmse_saliency_swap = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_saliency_swap))\n",
        "# mape_saliency_swap = mean_absolute_percentage_error(y_test_inv, y_test_pred_saliency_swap)\n",
        "\n",
        "# rmse_saliency_mean = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_saliency_mean))\n",
        "# mape_saliency_mean = mean_absolute_percentage_error(y_test_inv, y_test_pred_saliency_mean)\n",
        "\n",
        "# rmse_saliency_zero = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_saliency_zero))\n",
        "# mape_saliency_zero = mean_absolute_percentage_error(y_test_inv, y_test_pred_saliency_zero)\n",
        "\n",
        "# rmse_saliency_permute = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_saliency_permute))\n",
        "# mape_saliency_permute = mean_absolute_percentage_error(y_test_inv, y_test_pred_saliency_permute)\n",
        "\n",
        "# # Print the results\n",
        "# print(f\"Original Testing RMSE: {rmse_test}\")\n",
        "# print(f\"Original Testing MAPE: {mape_test}\")\n",
        "# print(f\"Testing RMSE after SHAP Swap: {rmse_shap_swap}\")\n",
        "# print(f\"Testing MAPE after SHAP Swap: {mape_shap_swap}\")\n",
        "# print(f\"Testing RMSE after SHAP Mean: {rmse_shap_mean}\")\n",
        "# print(f\"Testing MAPE after SHAP Mean: {mape_shap_mean}\")\n",
        "# print(f\"Testing RMSE after SHAP Zero: {rmse_shap_zero}\")\n",
        "# print(f\"Testing MAPE after SHAP Zero: {mape_shap_zero}\")\n",
        "# print(f\"Testing RMSE after SHAP Permute: {rmse_shap_permute}\")\n",
        "# print(f\"Testing MAPE after SHAP Permute: {mape_shap_permute}\")\n",
        "\n",
        "# print(f\"Testing RMSE after Saliency Swap: {rmse_saliency_swap}\")\n",
        "# print(f\"Testing MAPE after Saliency Swap: {mape_saliency_swap}\")\n",
        "# print(f\"Testing RMSE after Saliency Mean: {rmse_saliency_mean}\")\n",
        "# print(f\"Testing MAPE after Saliency Mean: {mape_saliency_mean}\")\n",
        "# print(f\"Testing RMSE after Saliency Zero: {rmse_saliency_zero}\")\n",
        "# print(f\"Testing MAPE after Saliency Zero: {mape_saliency_zero}\")\n",
        "# print(f\"Testing RMSE after Saliency Permute: {rmse_saliency_permute}\")\n",
        "# print(f\"Testing MAPE after Saliency Permute: {mape_saliency_permute}\")\n",
        "\n",
        "# # Plot the results for SHAP\n",
        "# metrics_shap = ['Original', 'Swap', 'Mean', 'Zero', 'Permute']\n",
        "# rmse_values_shap = [rmse_test, rmse_shap_swap, rmse_shap_mean, rmse_shap_zero, rmse_shap_permute]\n",
        "# mape_values_shap = [mape_test, mape_shap_swap, mape_shap_mean, mape_shap_zero, mape_shap_permute]\n",
        "\n",
        "# index = np.arange(len(metrics_shap))\n",
        "\n",
        "# fig, ax1 = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# bar_width = 0.35\n",
        "\n",
        "# bars1 = ax1.bar(index, rmse_values_shap, bar_width, color='blue', alpha=0.6, label='RMSE')\n",
        "# ax1.set_xlabel('SHAP Perturbation Methods')\n",
        "# ax1.set_ylabel('RMSE', color='blue')\n",
        "# ax1.set_title('Comparison of RMSE and MAPE across Original and Different SHAP Perturbation Methods')\n",
        "# ax1.set_xticks(index)\n",
        "# ax1.set_xticklabels(metrics_shap)\n",
        "\n",
        "# ax2 = ax1.twinx()\n",
        "# bars2 = ax2.bar(index + bar_width, mape_values_shap, bar_width, color='orange', alpha=0.6, label='MAPE')\n",
        "# ax2.set_ylabel('MAPE', color='orange')\n",
        "# ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "# ax1.legend([bars1, bars2], ['RMSE', 'MAPE'], loc='upper left')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # Plot the results for Saliency\n",
        "# metrics_saliency = ['Original', 'Swap', 'Mean', 'Zero', 'Permute']\n",
        "# rmse_values_saliency = [rmse_test, rmse_saliency_swap, rmse_saliency_mean, rmse_saliency_zero, rmse_saliency_permute]\n",
        "# mape_values_saliency = [mape_test, mape_saliency_swap, mape_saliency_mean, mape_saliency_zero, mape_saliency_permute]\n",
        "\n",
        "# index = np.arange(len(metrics_saliency))\n",
        "\n",
        "# fig, ax1 = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# bar_width = 0.35\n",
        "\n",
        "# bars1 = ax1.bar(index, rmse_values_saliency, bar_width, color='blue', alpha=0.6, label='RMSE')\n",
        "# ax1.set_xlabel('Saliency Perturbation Methods')\n",
        "# ax1.set_ylabel('RMSE', color='blue')\n",
        "# ax1.set_title('Comparison of RMSE and MAPE across Original and Different Saliency Perturbation Methods')\n",
        "# ax1.set_xticks(index)\n",
        "# ax1.set_xticklabels(metrics_saliency)\n",
        "\n",
        "# ax2 = ax1.twinx()\n",
        "# bars2 = ax2.bar(index + bar_width, mape_values_saliency, bar_width, color='orange', alpha=0.6, label='MAPE')\n",
        "# ax2.set_ylabel('MAPE', color='orange')\n",
        "# ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "# ax1.legend([bars1, bars2], ['RMSE', 'MAPE'], loc='upper left')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XAI done for HDFCBANK.NS\n"
          ]
        }
      ],
      "source": [
        "print(f'XAI done for {ticker}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
